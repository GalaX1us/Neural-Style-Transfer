{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "17_w9rQIS3h_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import imageio\n",
        "from skimage import color\n",
        "from abc import ABC, abstractmethod\n",
        "import tensorflow_probability as tfp\n",
        "#from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "F0b25njITrh6"
      },
      "outputs": [],
      "source": [
        "#Training Parameters\n",
        "TOLERANCE = 1e-5\n",
        "EPOCHS = 500\n",
        "\n",
        "#Weights\n",
        "LOSS_TYPE = 1\n",
        "STYLE_WEIGHTS = [1e0]\n",
        "CONTENT_WEIGHT = 0.05\n",
        "TOTAL_VARIATION_WEIGHT = 8.5e-5\n",
        "\n",
        "#Image\n",
        "MAX_IMG_SIZE = 512\n",
        "CONTENT_PATH = 'tubingen.jpg'\n",
        "STYLE_PATHS = ['starry-night.jpg']\n",
        "OUTPUT_PATH = './'\n",
        "\n",
        "#Layers\n",
        "STYLE_LAYERS = ['block1_conv1', 'block1_conv2', 'block2_conv1', 'block2_conv2', 'block3_conv1', 'block3_conv2', 'block3_conv3', 'block3_conv4',\n",
        "                    'block4_conv1', 'block4_conv2', 'block4_conv3', 'block4_conv4', 'block5_conv1', 'block5_conv2', 'block5_conv3', 'block5_conv4']\n",
        "\n",
        "CONTENT_LAYERS = ['block5_conv2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ckh7V-BuTXI4"
      },
      "outputs": [],
      "source": [
        "def imresize(img, size, interp='bilinear'):\n",
        "  \"\"\"\n",
        "  Used to resize images to a specific size\n",
        "\n",
        "  Args:\n",
        "      img (Image): image to resize\n",
        "      size (tuple): size\n",
        "      interp (str, optional): interpolation style. Defaults to 'bilinear'.\n",
        "\n",
        "  Returns:\n",
        "      _type_: _description_\n",
        "  \"\"\"\n",
        "  if interp == 'bilinear':\n",
        "      interpolation = Image.BILINEAR\n",
        "  elif interp == 'bicubic':\n",
        "      interpolation = Image.BICUBIC\n",
        "  else:\n",
        "      interpolation = Image.NEAREST\n",
        "\n",
        "  size = (size[1], size[0])\n",
        "\n",
        "  if type(img) != Image:\n",
        "      img = Image.fromarray(img, mode='RGB')\n",
        "\n",
        "  img = np.array(img.resize(size, interpolation))\n",
        "  return img\n",
        "    \n",
        "def imsave(path, img):\n",
        "  \"\"\"\n",
        "  Save an image\n",
        "\n",
        "  Args:\n",
        "      path (string): path where the image will be saved\n",
        "      img (Image): image to be saved\n",
        "  \"\"\"\n",
        "  imageio.imwrite(path, img)\n",
        "  return\n",
        "\n",
        "def load_img(path_to_img):\n",
        "  \"\"\"\n",
        "  Load an image from a path, resize it to match the maximum dim and format it\n",
        "  for training\n",
        "\n",
        "  Args:\n",
        "      path_to_img (string): path to the image\n",
        "\n",
        "  Returns:\n",
        "      numpy array: image as a np array\n",
        "  \"\"\"\n",
        "  max_dim = MAX_IMG_SIZE\n",
        "  img = tf.io.read_file(path_to_img)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "  \n",
        "  img *= 255\n",
        "\n",
        "  shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  img = tf.image.resize(img, new_shape)\n",
        "  img = img[tf.newaxis, :]\n",
        "  img = img.numpy()\n",
        "  \n",
        "  return img\n",
        "\n",
        "def load_and_process_img(path_to_img):\n",
        "  \"\"\"\n",
        "  Load and preprocess the image so it can be fed into the model\n",
        "\n",
        "  Args:\n",
        "      path_to_img (path): path to the image\n",
        "\n",
        "  Returns:\n",
        "      np array: preprocessed image\n",
        "  \"\"\"\n",
        "  img = load_img(path_to_img)\n",
        "  img = tf.keras.applications.vgg19.preprocess_input(img)\n",
        "  return img\n",
        "\n",
        "def deprocess_img(processed_img):\n",
        "  \"\"\"\n",
        "  Undo the preprocessing on the image\n",
        "\n",
        "  Args:\n",
        "      processed_img (np array): image to be deprocessed\n",
        "\n",
        "  Raises:\n",
        "      ValueError: Invalid input to deprocessing image\n",
        "\n",
        "  Returns:\n",
        "      np array: deprocessed image\n",
        "  \"\"\"\n",
        "  if len(processed_img.shape) == 4:\n",
        "    processed_img = np.squeeze(processed_img, 0)\n",
        "  assert len(processed_img.shape) == 3, (\"Input to deprocess image must be an image of \"\n",
        "                             \"dimension [1, height, width, channel] or [height, width, channel]\")\n",
        "  if len(processed_img.shape) != 3:\n",
        "    raise ValueError(\"Invalid input to deprocessing image\")\n",
        "  \n",
        "  # perform the inverse of the preprocessing step\n",
        "  processed_img[:, :, 0] += 103.939\n",
        "  processed_img[:, :, 1] += 116.779\n",
        "  processed_img[:, :, 2] += 123.68\n",
        "  processed_img = processed_img[:, :, ::-1]\n",
        "\n",
        "  processed_img = np.clip(processed_img, 0, 255).astype('uint8')\n",
        "  return processed_img\n",
        "\n",
        "def clip_image(image):\n",
        "  return tf.clip_by_value(image, clip_value_min=-128, clip_value_max=128.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vlTNs8JZTgNT"
      },
      "outputs": [],
      "source": [
        "# Ported from https://pychao.com/2019/11/02/optimize-tensorflow-keras-models-with-l-bfgs-from-tensorflow-probability/\n",
        "class AbstractTFPOptimizer(ABC):\n",
        "\n",
        "    def __init__(self, trace_function=False):\n",
        "        super(AbstractTFPOptimizer, self).__init__()\n",
        "        self.trace_function = trace_function\n",
        "        self.callback_list = None\n",
        "\n",
        "    def _function_wrapper(self, loss_func, model):\n",
        "        \"\"\"A factory to create a function required by tfp.optimizer.lbfgs_minimize.\n",
        "        Args:\n",
        "            loss_func: a function with signature loss_value = loss(model).\n",
        "            model: an instance of `tf.keras.Model` or its subclasses.\n",
        "        Returns:\n",
        "            A function that has a signature of:\n",
        "                loss_value, gradients = f(model_parameters).\n",
        "        \"\"\"\n",
        "\n",
        "        # obtain the shapes of all trainable parameters in the model\n",
        "        shapes = tf.shape_n(model.trainable_variables)\n",
        "        n_tensors = len(shapes)\n",
        "\n",
        "        # we'll use tf.dynamic_stitch and tf.dynamic_partition later, so we need to\n",
        "        # prepare required information first\n",
        "        count = 0\n",
        "        idx = []  # stitch indices\n",
        "        part = []  # partition indices\n",
        "\n",
        "        for i, shape in enumerate(shapes):\n",
        "            n = np.product(shape)\n",
        "            idx.append(tf.reshape(tf.range(count, count + n, dtype=tf.int32), shape))\n",
        "            part.extend([i] * n)\n",
        "            count += n\n",
        "\n",
        "        part = tf.constant(part)\n",
        "\n",
        "        @tf.function\n",
        "        def assign_new_model_parameters(params_1d):\n",
        "            \"\"\"A function updating the model's parameters with a 1D tf.Tensor.\n",
        "            Args:\n",
        "                params_1d [in]: a 1D tf.Tensor representing the model's trainable parameters.\n",
        "            \"\"\"\n",
        "\n",
        "            params = tf.dynamic_partition(params_1d, part, n_tensors)\n",
        "            for i, (shape, param) in enumerate(zip(shapes, params)):\n",
        "                model.trainable_variables[i].assign(tf.reshape(param, shape))\n",
        "\n",
        "        # now create a function that will be returned by this factory\n",
        "        def f(params_1d):\n",
        "            \"\"\"A function that can be used by tfp.optimizer.lbfgs_minimize.\n",
        "            This function is created by function_factory.\n",
        "            Args:\n",
        "               params_1d [in]: a 1D tf.Tensor.\n",
        "            Returns:\n",
        "                A scalar loss and the gradients w.r.t. the `params_1d`.\n",
        "            \"\"\"\n",
        "\n",
        "            # use GradientTape so that we can calculate the gradient of loss w.r.t. parameters\n",
        "            with tf.GradientTape() as tape:\n",
        "                # update the parameters in the model\n",
        "                assign_new_model_parameters(params_1d)\n",
        "                # calculate the loss\n",
        "                loss_value = loss_func(model)\n",
        "\n",
        "            # calculate gradients and convert to 1D tf.Tensor\n",
        "            grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "            grads = tf.dynamic_stitch(idx, grads)\n",
        "\n",
        "            # print out iteration & loss\n",
        "            f.iter.assign_add(1)\n",
        "            tf.print(\"Iter:\", f.iter, \"loss:\", loss_value)\n",
        "\n",
        "            if self.callback_list is not None:\n",
        "                info_dict = {\n",
        "                    'iter': f.iter,\n",
        "                    'loss': loss_value,\n",
        "                    'grad': grads,\n",
        "                }\n",
        "\n",
        "                for callback in self.callback_list:\n",
        "                    callback(model, info_dict=info_dict)\n",
        "\n",
        "            return loss_value, grads\n",
        "\n",
        "        if self.trace_function:\n",
        "            f = tf.function(f)\n",
        "\n",
        "        # store these information as members so we can use them outside the scope\n",
        "        f.iter = tf.Variable(0, trainable=False)\n",
        "        f.idx = idx\n",
        "        f.part = part\n",
        "        f.shapes = shapes\n",
        "        f.assign_new_model_parameters = assign_new_model_parameters\n",
        "\n",
        "        return f\n",
        "\n",
        "    def register_callback(self, callable):\n",
        "        \"\"\"\n",
        "        Accepts a callable with signature `callback(model, info_dict=None)`.\n",
        "        Callable should not return anything, it will not be dealt with.\n",
        "        `info_dict` will contain the following information:\n",
        "            - Optimizer iteration number (key = 'iter')\n",
        "            - Loss value (key = 'loss')\n",
        "            - Grad value (key = 'grad')\n",
        "        Args:\n",
        "            callable: A callable function with the signature `callable(model, info_dict=None)`.\n",
        "            See above for what info_dict can contain.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.callback_list is None:\n",
        "            self.callback_list = []\n",
        "\n",
        "        self.callback_list.append(callable)\n",
        "\n",
        "    @abstractmethod\n",
        "    def minimize(self, loss_func, model):\n",
        "        pass\n",
        "\n",
        "\n",
        "class BFGSOptimizer(AbstractTFPOptimizer):\n",
        "\n",
        "    def __init__(self, max_iterations=50, tolerance=1e-8, bfgs_kwargs=None, trace_function=False):\n",
        "        super(BFGSOptimizer, self).__init__(trace_function=trace_function)\n",
        "\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "\n",
        "        bfgs_kwargs = bfgs_kwargs or {}\n",
        "\n",
        "        if 'max_iterations' in bfgs_kwargs.keys():\n",
        "            del bfgs_kwargs['max_iterations']\n",
        "\n",
        "        if 'tolerance' in bfgs_kwargs.keys():\n",
        "            keys = [key for key in bfgs_kwargs.keys()\n",
        "                    if 'tolerance' in key]\n",
        "            for key in keys:\n",
        "                del bfgs_kwargs[key]\n",
        "\n",
        "        self.bfgs_kwargs = bfgs_kwargs\n",
        "\n",
        "    def minimize(self, loss_func, model):\n",
        "        optim_func = self._function_wrapper(loss_func, model)\n",
        "\n",
        "        # convert initial model parameters to a 1D tf.Tensor\n",
        "        init_params = tf.dynamic_stitch(optim_func.idx, model.trainable_variables)\n",
        "\n",
        "        # train the model with BFGS solver\n",
        "        results = tfp.optimizer.bfgs_minimize(\n",
        "            value_and_gradients_function=optim_func, initial_position=init_params,\n",
        "            max_iterations=self.max_iterations,\n",
        "            tolerance=self.tolerance,\n",
        "            x_tolerance=self.tolerance,\n",
        "            f_relative_tolerance=self.tolerance,\n",
        "            **self.bfgs_kwargs)\n",
        "\n",
        "        # after training, the final optimized parameters are still in results.position\n",
        "        # so we have to manually put them back to the model\n",
        "        optim_func.assign_new_model_parameters(results.position)\n",
        "\n",
        "        print(\"BFGS complete, and parameters updated !\")\n",
        "        return model\n",
        "\n",
        "\n",
        "class LBFGSOptimizer(AbstractTFPOptimizer):\n",
        "\n",
        "    def __init__(self, max_iterations=50, tolerance=1e-8, lbfgs_kwargs=None, trace_function=False):\n",
        "        super(LBFGSOptimizer, self).__init__(trace_function=trace_function)\n",
        "\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "\n",
        "        lbfgs_kwargs = lbfgs_kwargs or {}\n",
        "\n",
        "        if 'max_iterations' in lbfgs_kwargs.keys():\n",
        "            del lbfgs_kwargs['max_iterations']\n",
        "\n",
        "        if 'tolerance' in lbfgs_kwargs.keys():\n",
        "            keys = [key for key in lbfgs_kwargs.keys()\n",
        "                    if 'tolerance' in key]\n",
        "            for key in keys:\n",
        "                del lbfgs_kwargs[key]\n",
        "\n",
        "        self.lbfgs_kwargs = lbfgs_kwargs\n",
        "\n",
        "    def minimize(self, loss_func, model):\n",
        "        optim_func = self._function_wrapper(loss_func, model)\n",
        "\n",
        "        # convert initial model parameters to a 1D tf.Tensor\n",
        "        init_params = tf.dynamic_stitch(optim_func.idx, model.trainable_variables)\n",
        "\n",
        "        # train the model with L-BFGS solver\n",
        "        results = tfp.optimizer.lbfgs_minimize(\n",
        "            value_and_gradients_function=optim_func, initial_position=init_params,\n",
        "            max_iterations=self.max_iterations,\n",
        "            tolerance=self.tolerance,\n",
        "            x_tolerance=self.tolerance,\n",
        "            f_relative_tolerance=self.tolerance,\n",
        "            **self.lbfgs_kwargs)\n",
        "\n",
        "        # after training, the final optimized parameters are still in results.position\n",
        "        # so we have to manually put them back to the model\n",
        "        optim_func.assign_new_model_parameters(results.position)\n",
        "\n",
        "        print(\"L-BFGS complete, and parameters updated !\")\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "swPQEgNHT4JS"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(x):\n",
        "    \"\"\"\n",
        "    the gram matrix of an image tensor (feature-wise outer product) using shifted activations\n",
        "\n",
        "    Args:\n",
        "        x (np array): image to apply gram matrix on\n",
        "\n",
        "    Returns:\n",
        "        np array: transformed image\n",
        "    \"\"\"\n",
        "    gram = tf.linalg.einsum('bijc,bijd->bcd', x - 1, x - 1)\n",
        "    return gram\n",
        "\n",
        "class StyleContentModel(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Allows to have the output of each layer for a specific  input\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, style_layers, content_layers):\n",
        "        super(StyleContentModel, self).__init__()\n",
        "\n",
        "        # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "        transferL = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "        transferL.trainable = False\n",
        "        \n",
        "        outputs_dict = dict([(layer.name, layer.output) for layer in transferL.layers])\n",
        "\n",
        "        style_activations = [outputs_dict[layer_name] for layer_name in style_layers]\n",
        "        content_activations = [outputs_dict[layer_name] for layer_name in content_layers]\n",
        "\n",
        "        activations = style_activations + content_activations\n",
        "\n",
        "        self.vgg = tf.keras.Model(transferL.input, activations)\n",
        "\n",
        "        self.style_layer_names = style_layers\n",
        "        self.content_layer_names = content_layers\n",
        "\n",
        "        self.num_style_layers = len(style_layers)\n",
        "        self.num_content_layers = len(content_layers)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        outputs = self.vgg(inputs)\n",
        "        style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
        "                                          outputs[self.num_style_layers:])\n",
        "\n",
        "        style_outputs = [gram_matrix(style_output)\n",
        "                         for style_output in style_outputs]\n",
        "\n",
        "        content_dict = {content_name: value\n",
        "                        for content_name, value\n",
        "                        in zip(self.content_layer_names, content_outputs)}\n",
        "\n",
        "        style_dict = {style_name: value\n",
        "                      for style_name, value\n",
        "                      in zip(self.style_layer_names, style_outputs)}\n",
        "\n",
        "        return {'content': content_dict, 'style': style_dict}\n",
        "    \n",
        "def style_loss(style, combination, size):\n",
        "    \"\"\"\n",
        "    Compute the style loss\n",
        "\n",
        "    Args:\n",
        "        style (np array): target image\n",
        "        combination (np array): trained image\n",
        "        size (tuple): size of the image\n",
        "\n",
        "    Returns:\n",
        "        np array: loss value\n",
        "    \"\"\"\n",
        "    channels = 3\n",
        "    return tf.reduce_sum(tf.square(style - combination)) / (4. * (channels ** 2) * (size ** 2))\n",
        "\n",
        "def content_loss(base, combination, size):\n",
        "    \"\"\"\n",
        "    Compute content loss\n",
        "\n",
        "    Args:\n",
        "        base (np array): target image\n",
        "        combination (np array): trained image\n",
        "        size (tuple): size of the image\n",
        "\n",
        "    Returns:\n",
        "        np array: loss value\n",
        "    \"\"\"\n",
        "    channels = 3\n",
        "    if LOSS_TYPE == 1:\n",
        "        multiplier = 1. / (2. * (channels ** 0.5) * (size ** 0.5))\n",
        "    elif LOSS_TYPE == 2:\n",
        "        multiplier = 1. / (channels * size)\n",
        "    else:\n",
        "        multiplier = 1.\n",
        "\n",
        "    return multiplier * tf.reduce_sum(tf.square(combination - base))\n",
        "\n",
        "# \n",
        "def total_variation_loss(x):\n",
        "    \"\"\"\n",
        "    The total variation loss is designed to keep the generated image locally coherent\n",
        "    by reducing high frequency artifacts\n",
        "\n",
        "    Args:\n",
        "        x (np array): input image\n",
        "\n",
        "    Returns:\n",
        "        np array: loss value\n",
        "    \"\"\"\n",
        "    a = tf.square(\n",
        "        x[:, :-1, :-1, :] - x[:, 1:, :-1, :]\n",
        "    )\n",
        "    b = tf.square(\n",
        "        x[:, :-1, :-1, :] - x[:, :-1, 1:, :]\n",
        "    )\n",
        "    return tf.reduce_sum(tf.pow(a + b, 1.25))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_loss(input, outputs, content_target, style_targets):\n",
        "    \"\"\"\n",
        "    Compute the overall loss of the image\n",
        "\n",
        "    Args:\n",
        "        input (np array): trained image\n",
        "        outputs (dict): outputs of each layer of the model with this input\n",
        "        content_target (np array): self explanatory\n",
        "        style_targets (np array): self explanatory\n",
        "\n",
        "    Returns:\n",
        "        np array: overall loss value\n",
        "    \"\"\"\n",
        "    style_combined_outputs = outputs['style']\n",
        "    content_combined_outputs = outputs['content']\n",
        "    h,w,c = input.shape[1:]\n",
        "    size = h*w\n",
        "\n",
        "    # Content losses\n",
        "    content_losses = CONTENT_WEIGHT * tf.add_n([content_loss(content_target[name], content_combined_outputs[name], size)\n",
        "                                                            for name in content_combined_outputs.keys()])\n",
        "\n",
        "    num_style_layers = len(STYLE_LAYERS)\n",
        "    num_style_references = len(style_targets)\n",
        "\n",
        "    # Style losses (Cross layer loss)\n",
        "    style_losses = []\n",
        "    for style_img_id in range(num_style_references):\n",
        "        style_features = style_targets[style_img_id]\n",
        "\n",
        "        sl_i = 0.\n",
        "        for feature_layer_id in range(num_style_layers - 1):\n",
        "            target_feature_layer = style_features[STYLE_LAYERS[feature_layer_id]]\n",
        "            style_output = style_combined_outputs[STYLE_LAYERS[feature_layer_id]]\n",
        "\n",
        "            sl1 = style_loss(target_feature_layer, style_output, size)\n",
        "\n",
        "            target_feature_layer = style_features[STYLE_LAYERS[feature_layer_id + 1]]\n",
        "            style_output = style_combined_outputs[STYLE_LAYERS[feature_layer_id + 1]]\n",
        "\n",
        "            sl2 = style_loss(target_feature_layer, style_output, size)\n",
        "\n",
        "            # Geometric loss scaling\n",
        "            sl_i = sl_i + (sl1 - sl2) * (STYLE_WEIGHTS[style_img_id] / (2 ** (num_style_layers - 1 - (feature_layer_id + 1))))\n",
        "\n",
        "        style_losses.append(sl_i)\n",
        "\n",
        "    style_losses = tf.add_n(style_losses)\n",
        "\n",
        "    # Total Variation Losses\n",
        "    tv_losses = TOTAL_VARIATION_WEIGHT * total_variation_loss(input)\n",
        "\n",
        "    return content_losses, style_losses, tv_losses\n",
        "\n",
        "def get_feature_representations(model, content_path, style_paths):\n",
        "  \"\"\"Helper function to compute our content and style feature representations.\n",
        "\n",
        "  This function will simply load and preprocess both the content and style \n",
        "  images from their path. Then it will feed them through the network to obtain\n",
        "  the outputs of the intermediate layers. \n",
        "  \n",
        "  Arguments:\n",
        "    model: The model that we are using.\n",
        "    content_path: The path to the content image.\n",
        "    style_path: The path to the style image\n",
        "    \n",
        "  Returns:\n",
        "    returns the style features and the content features. \n",
        "  \"\"\"\n",
        "  \n",
        "  content_image = load_and_process_img(content_path)\n",
        "  content_features = model(content_image)['content']\n",
        "  \n",
        "  style_features = []\n",
        "  for path in style_paths: \n",
        "    style_image = load_and_process_img(path)\n",
        "    style_features.append(model(style_image)['style'])\n",
        "\n",
        "  return style_features, content_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3_6Irr2UBEd",
        "outputId": "c0c144ce-1155-470f-dba0-7ab0443fab6b"
      },
      "outputs": [],
      "source": [
        "extractor = StyleContentModel(style_layers=STYLE_LAYERS, content_layers=CONTENT_LAYERS)\n",
        "\n",
        "base_image = load_and_process_img(CONTENT_PATH)\n",
        "\n",
        "style_targets_list, content_target = get_feature_representations(extractor, CONTENT_PATH, STYLE_PATHS)\n",
        "\n",
        "x = tf.Variable(base_image, trainable=True)\n",
        "\n",
        "class InputWrapper(tf.keras.Model):\n",
        "    def __init__(self, x: tf.Variable):\n",
        "        super(InputWrapper, self).__init__()\n",
        "\n",
        "        self.x = x\n",
        "\n",
        "        self.vgg = extractor\n",
        "        self.vgg.trainable = False\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        outputs = self.vgg(self.x)\n",
        "        return outputs\n",
        "\n",
        "x_wrapper = InputWrapper(x)\n",
        "\n",
        "@tf.function  # (tracing will be done by LBFGS for us)\n",
        "def loss_wrapper(model):\n",
        "    outputs = model(x)\n",
        "    content_losses, style_losses, tv_losses = compute_loss(x, outputs, content_target, style_targets_list)\n",
        "    loss = content_losses + style_losses + tv_losses\n",
        "    return loss\n",
        "\n",
        "prev_min_val = -1\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "def save_image_callback(model, info_dict=None):\n",
        "    \"\"\"\n",
        "    Callback function to get infos while training\n",
        "    \"\"\"\n",
        "    global prev_min_val, start_time\n",
        "\n",
        "    info_dict = info_dict or {}\n",
        "    loss_value = info_dict.get('loss', None)\n",
        "    i = info_dict.get('iter', -1)\n",
        "\n",
        "    if loss_value is not None:\n",
        "        loss_val = loss_value.numpy()\n",
        "\n",
        "        if prev_min_val == -1:\n",
        "            prev_min_val = loss_val\n",
        "\n",
        "        improvement = (prev_min_val - loss_val) / prev_min_val * 100\n",
        "\n",
        "        print(\"Current loss value:\", loss_val, \" Improvement : %0.3f\" % improvement, \"%\")\n",
        "        prev_min_val = loss_val\n",
        "\n",
        "    last_save = info_dict.get('last_save', False)\n",
        "    if (i + 1) % 100 == 0 or last_save:\n",
        "        img = model.x.numpy()\n",
        "        # save current generated image\n",
        "        img = deprocess_img(img)\n",
        "\n",
        "        if not last_save:\n",
        "            fname = OUTPUT_PATH + \"out\" + \"_at_iteration_%d.png\" % (i + 1)\n",
        "        else:\n",
        "            fname = OUTPUT_PATH + \"out\" + \"_final.png\"\n",
        "\n",
        "        imsave(fname, img)\n",
        "        end_time = time.time()\n",
        "        print(\"Image saved as\", fname)\n",
        "        print(\"Iteration %d completed in %ds\" % (i + 1, end_time - start_time))\n",
        "\n",
        "\n",
        "optimizer = LBFGSOptimizer(max_iterations=EPOCHS, tolerance=TOLERANCE)\n",
        "\n",
        "# Add a save image callback to this\n",
        "optimizer.register_callback(save_image_callback)\n",
        "\n",
        "optimizer.minimize(loss_wrapper, x_wrapper)\n",
        "\n",
        "save_image_callback(x_wrapper, info_dict={'last_save': True})"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.4 (tags/v3.9.4:1f2e308, Apr  4 2021, 13:27:16) [MSC v.1928 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "8b34c6dc9c2ebd3a680ced2d1389fea591586cdef662a10b49081ba69c2d80ae"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
